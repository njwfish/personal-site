<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<p>I was reading Amartya Sen’s exposition of Buridan’s ass in his
excellent essay “Behaviour and the Concept of Preference” and it got me
thinking about the revealed preference approach with computationally
bounded agents. Sen writes:</p>
<blockquote>
<p>[Buridan’s] ass, as we all know, could not make up its mind between
two haystacks; it liked both very much but could not decide which one
was better. Being unable to choose, this dithering animal died
ultimately of starvation. The dilemma of the ass is easy to understand,
but it is possible that the animal would have agreed that it would have
been better off by choosing either of the haystacks rather than nothing
at all. Not choosing anything is also a choice, and in this case this
really meant the choice of starvation. On the other hand, had it chosen
either of the haystacks, it would have been presumed that the ass
regarded that haystack to be at least as good as the other, which in
this version of the story was not the case. The ass was in a real
dilemma vis-a-vis the revealed preference approach.</p>
</blockquote>
<p>The core idea I’m interested in here is whether computational bounds
restrict our ability to learn agents actual preferences since agents do
not actually make optimal decisions in practice. In this short post I
mostly just pose questios thinking about this issue through a classic
NP-hard problem (the knapsack problem), but the particular problem isn’t
really relevant. Really I start from the premises that:</p>
<ol type="1">
<li>there is private preference information we want to infer from
behaviour alone<br />
</li>
<li>the agent is trying to solve a “hard” and so they approximate a
solution rather than solving it optimally</li>
<li>we can repeatedly ask the agent to solve variants of the problem to
observe their behaviour in different situations</li>
</ol>
<p>and ask how well and how quickly we can learn the agents private
preferences (contrasted against the case where we know the agent is
playing optimally) and taking the ass’s perspective, so-to-speak, we can
also ask whether there are strategies an agent could deploy in this
repeated context which could signal its true preferences under
computational constraints.</p>
<hr />
<p>Assume we have an agent which maximizes their utility under a budget
constraint <span class="math inline">\(B\)</span> over a set <span
class="math inline">\(K\)</span> items denoted <span
class="math inline">\(S\)</span> with costs <span
class="math inline">\(c_1, \dots, c_K\)</span> and private utilities
<span class="math inline">\(u_1, \dots, u_K\)</span>. That is the agent
optimizes the set <span class="math inline">\(I\)</span>:</p>
<p><span class="math display">\[
\begin{align}
I^* = \max_{I \subseteq S} &amp;\sum_{i \in I} u_i \\
\text{ s.t. } &amp;\sum_{i \in I} c_i \leq B
\end{align}
\]</span></p>
<p>That is the agent is playing a knapsack problem. We can observe the
costs of each item, the budget the agent operates under, and the set the
agent selects <span class="math inline">\(I^*\)</span>. We cannot
observe the agents private utilities.</p>
<p>Now let’s assume we can query the agent with a given budget <span
class="math inline">\(b \in \mathbb{R}\)</span> and a given subset of
elements <span class="math inline">\(s \subseteq S\)</span> and observe
their response <span class="math inline">\(I^*(s, b)\)</span>. Note that
in general we cannot recover an agent’s exact private utilities:
consider the case where <span class="math inline">\(S = \{(w_1=1,
u_1=1), (w_2=1,u_2=0)\}\)</span>. Here we can only recover the fact that
<span class="math inline">\(u_1 &gt; u_2\)</span> but cannot gain any
more information. We must set our sights slightly lower then and
restrict our interest to being able to exactly reconstruct the function
<span class="math inline">\(I^*\)</span> over all possible sets and
budgets.</p>
<h5 id="what-is-the-query-complexity-to-exactly-recover-i">1. What is
the query complexity to exactly recover <span
class="math inline">\(I^*\)</span>?</h5>
<p>This is clearly a question in the tradition of dicrete choice theory
and computation and is interesting in its own right. It is far from
trivial! Obviously we could query the product of all possible subsets
and all possible budgets but it seems we should be able to do better
than this given our knowledge of the problem.</p>
<p>In the spirit of Buridan’s ass I want to go further though and asking
another question. We know the optimization variant of the knapsack
problem is at least NP-hard, so we might not expect our agent to be able
to compute the optimal <span class="math inline">\(I^*\)</span>. Instead
we might assume that our agent uses a deterministic unknown
polynomial-time algorithm <span class="math inline">\(A: \mathcal{P}(S)
\times \mathbb{R} \rightarrow \mathcal{P}(S)\)</span> to select their
set rather than always solving the full knapsack problem above. We will
assume <span class="math inline">\(A\)</span> is in a known class of
algorithms <span class="math inline">\(\mathcal{A}\)</span> Now we can
pose the really interesting questions:</p>
<h5
id="under-what-conditions-on-mathcala-can-we-characterize-the-query-complexity-to-exactly-recover-a">2.
Under what conditions on <span
class="math inline">\(\mathcal{A}\)</span> can we characterize the query
complexity to exactly recover <span
class="math inline">\(A\)</span>?</h5>
<p>This question what classes of approximation are query-recoverable in
better than worst case (enumerate all subsets and all budgets) time. For
example if we know <span class="math inline">\(A\)</span> is a
1/2-approximation (e.g. the selected set is always at least half as good
as the optimal set) then maybe we can do better than worst case query
complexity. Or maybe not. It is not obvious to me!</p>
<h5
id="under-what-conditions-on-mathcala-can-we-recover-i-from-queries-to-the-agent">3.
Under what conditions on <span
class="math inline">\(\mathcal{A}\)</span> can we recover <span
class="math inline">\(I^*\)</span> from queries to the agent?</h5>
<p>This is of course the big question. Maybe the most interesting
starting place would be to consider with some arbitrarily small <span
class="math inline">\(\epsilon \in (0, 1]\)</span> and let <span
class="math inline">\(\mathcal{A} = A\)</span> be a <em>known</em> <span
class="math inline">\((1-\epsilon)\)</span>-approximation where the
selected set is always at least <span
class="math inline">\(1-\epsilon\)</span> as good as the optimal set. In
this highly restricted case can we recover <span
class="math inline">\(I^*\)</span>? This leads to the next question:</p>
<h5
id="given-a-class-of-algorithms-mathcala-what-is-the-best-1-epsilon-approximation-we-can-recover-for-the-worst-case-a-in-mathcala">4.
Given a class of algorithms <span
class="math inline">\(\mathcal{A}\)</span> what is the best <span
class="math inline">\((1-\epsilon)\)</span>-approximation we can recover
for the worst-case <span class="math inline">\(A \in
\mathcal{A}\)</span>?</h5>
<p>Here we relax our dreams of recovering <span
class="math inline">\(I^*\)</span> and instead ask what the best
recoverable approximation will be for a given class. And of course there
are query complexity analogues of both (2) and (3).</p>
<p>These questions are quite important from the perspective of what we
can learn about the preferences of computaionally bounded agents from
their behavior alone. My intuition is that there are more negative than
positive results here, but those negative results are important! It
would imply that revealed preference approaches are seriously undermined
by computational constraints. And of course positive results would also
be welcome as they might lead to better econometric methods for studying
people’s preferences.</p>
<p>Additionally we can flip the question on its head and ask whether
there are ways an agent can select items to signal their preferences to
an observer:</p>
<h5
id="if-we-are-an-agent-and-we-know-there-is-a-computationally-unbounded-observer-trying-to-understand-our-preferences-via-their-queries-can-we-somehow-signal-our-true-preferences-via-our-selections-under-our-computational-constraints">5.
If we are an agent and we know there is a computationally unbounded
observer trying to understand our preferences via their queries can we
somehow signal our true preferences via our selections under our
computational constraints?</h5>
<p>This last question is the least well formed of the four, but it would
be particularly interesting if there were effective strategies
computationally bounded agents could use to signal their preferences,
especially if they involved deviating from the utility maximizing
strategy!</p>
<hr />
<p>In any case I think this is a super interesting direction for folks
at the intersection of economics and computation to think in, offering
both interesting puzzles and relevant conclusions. I have not seen any
work doing something like this, but if anyone knows of anything please
let me know!</p>
<br />
<br />
